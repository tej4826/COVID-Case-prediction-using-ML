{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7)\n",
      "(34, 10)\n",
      "(34, 10)\n",
      "[ 0.33212241 -0.50802577  0.37157664 -0.12121541  0.05964252 -0.10215886\n",
      "  0.01382647 -0.0057703   0.48489211  0.3408232 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (34,9) and (10,) not aligned: 9 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9df3500f6a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m#test_X_file_path = sys.argv[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0mtest_X_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X_FILE.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m     \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[1;31m# Uncomment to test on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_test_Y_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Y_FILE.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-9df3500f6a7d>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(test_X_file_path)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0mtest_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[0mpred_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_target_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m     \u001b[0mwrite_to_csv_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predicted_test_Y_pr.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-9df3500f6a7d>\u001b[0m in \u001b[0;36mpredict_target_values\u001b[1;34m(test_X, weights)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m#b=weights[i][-1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mZ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (34,9) and (10,) not aligned: 9 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from validate import validate\n",
    "\n",
    "\"\"\"\n",
    "Predicts the target values for data in the file at 'test_X_file_path', using the weights learned during training.\n",
    "Writes the predicted values to the file named \"predicted_test_Y_pr.csv\". It should be created in the same directory where this code file is present.\n",
    "This code is provided to help you get started and is NOT a complete implementation. Modify it based on the requirements of the project.\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    return s\n",
    "\n",
    "def import_data_and_weights(test_X_file_path, weights_file_path):\n",
    "    test_X = np.genfromtxt(test_X_file_path, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "    weights = np.genfromtxt(weights_file_path, delimiter=',', dtype=np.float64)\n",
    "    return test_X, weights\n",
    "\n",
    "def apply_one_hot_encoding(X):\n",
    "    unique_values = list(set(X))\n",
    "    unique_values.sort()\n",
    "    one_hot_encoding_map = {}\n",
    "    counter = 0\n",
    "    for x in unique_values:\n",
    "        one_hot_encoding_map[x] = [0 for i in range(len(unique_values))]\n",
    "        one_hot_encoding_map[x][counter] = 1\n",
    "        counter += 1\n",
    "    one_hot_encoded_X = []\n",
    "    for x in X:\n",
    "        one_hot_encoded_X.append(one_hot_encoding_map[x])\n",
    "\n",
    "    one_hot_encoded_X = np.array(one_hot_encoded_X, dtype=int)\n",
    "    return one_hot_encoded_X\n",
    "\n",
    "def convert_given_cols_to_one_hot(X, column_indices):\n",
    "    one_hot_encoded_X = np.zeros([len(X),1])\n",
    "\n",
    "    start_index = 0\n",
    "    #acts column pointer in X\n",
    "\n",
    "    for curr_index in column_indices:\n",
    "        #adding the columns present before curr_index in X (and not present in one_hot_encoded_X), to one_hot_encoded_X\n",
    "        one_hot_encoded_X=np.append(one_hot_encoded_X,X[:, start_index:curr_index], axis=1)\n",
    "        \n",
    "        #applying one hot encoding for current column\n",
    "        one_hot_encoded_column = apply_one_hot_encoding(X[:,curr_index])\n",
    "\n",
    "        #appending the obtained one hot encoded array to one_hot_encoded_X\n",
    "        one_hot_encoded_X=np.append(one_hot_encoded_X,one_hot_encoded_column, axis=1)\n",
    "\n",
    "        #moving the column pointer of X to next current_index\n",
    "        start_index = curr_index+1\n",
    "\n",
    "    #adding any remaining columns to one_hot_encoded_X    \n",
    "    one_hot_encoded_X=np.append(one_hot_encoded_X,X[:,start_index:], axis=1)\n",
    "    one_hot_encoded_X = one_hot_encoded_X[:,1:]\n",
    "    return one_hot_encoded_X\n",
    "\n",
    "\n",
    "def get_correlation_matrix(X):\n",
    "    num_vars = len(X[0])\n",
    "    m = len(X)\n",
    "    correlation_matix = np.zeros((num_vars,num_vars))\n",
    "    for i in range(0,num_vars):\n",
    "        for j in range(i,num_vars):\n",
    "            mean_i = np.mean(X[:,i])\n",
    "            mean_j = np.mean(X[:,j])\n",
    "            std_dev_i = np.std(X[:,i])\n",
    "            std_dev_j = np.std(X[:,j])\n",
    "            numerator = np.sum((X[:,i]-mean_i)*(X[:,j]-mean_j))\n",
    "            denominator = (m)*(std_dev_i)*(std_dev_j)\n",
    "            corr_i_j = numerator/denominator    \n",
    "            correlation_matix[i][j] = corr_i_j\n",
    "            correlation_matix[j][i] = corr_i_j\n",
    "    return correlation_matix\n",
    "\n",
    "def select_features(corr_mat, T1, T2):\n",
    "    n=len(corr_mat)\n",
    "    filtered_features = []\n",
    "    for i in range(1,n):\n",
    "        if (abs(corr_mat[i][0]) > T1):\n",
    "            filtered_features.append(i)\n",
    "    m = len(filtered_features)\n",
    "    removed_features = []\n",
    "    selected_features = list(filtered_features)\n",
    "    for i in range(0,m):\n",
    "        for j in range(i+1,m):\n",
    "            f1 = filtered_features[i]\n",
    "            f2 = filtered_features[j]\n",
    "            if (f1 not in removed_features and f2 not in removed_features): \n",
    "                if (abs(corr_mat[f1][f2]) > T2):\n",
    "                    selected_features.remove(f2)\n",
    "                    removed_features.append(f2)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "def replace_null_values_with_mean(X,real_X):\n",
    "    #Obtain mean of columns\n",
    "    col_mean = np.nanmean(real_X, axis=0)\n",
    "\n",
    "    #Find indicies that we need to replace\n",
    "    inds = np.where(np.isnan(X))\n",
    "\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    X[inds] = np.take(col_mean, inds[1])\n",
    "    return X\n",
    "\n",
    "def mean_normalize(X, column_indices):\n",
    "    X[:,column_indices]=((X[:,column_indices]-np.mean(X,axis=0)[column_indices])/((np.max(X,axis=0)[column_indices]-np.min(X,axis=0)[column_indices])))\n",
    "    return X\n",
    "\n",
    "def predict_target_values(test_X, weights):\n",
    "    k=[]\n",
    "    s=[]\n",
    "    test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "    print(weights[0][:])\n",
    "    for i in range(2):\n",
    "        #b=weights[i][-1]\n",
    "        \n",
    "        Z=np.dot(test_X,weights[i])\n",
    "        A=sigmoid(Z)\n",
    "        k.append(A)\n",
    "    k=np.array(k)\n",
    "    size=(k.shape)[1]\n",
    "    for i in range(size):\n",
    "        m=max(k[0][i],k[1][i])\n",
    "        if m==k[0][i]:\n",
    "            s.append([0])\n",
    "        else:\n",
    "            s.append([1])\n",
    "    s=np.array(s)\n",
    "    return s\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    The preprocessing techniques which are used on the train data, should also be applied on the test \n",
    "    1. The feature scaling technique used on the training data should be applied as it is (with same mean/standard deviation/min/max) on the test data as well.\n",
    "    2. The one-hot encoding mapping applied on the train data should also be applied on test data during prediction.\n",
    "    3. During training, you have to write any such values (mentioned in above points) to a file, so that they can be used for prediction.\n",
    "     \n",
    "    You can load the weights/parameters and the above mentioned preprocessing parameters, by reading them from a csv file which is present in the preprocessing_regularization.zip\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict Target Variables\n",
    "    \"\"\"\n",
    "    You can make use of any other helper functions which might be needed.\n",
    "    Make sure all such functions are submitted in preprocessing_regularization.zip and imported properly.\n",
    "    \"\"\"\n",
    "def min_max_normalize(X, column_indices):\n",
    "    for column_index in column_indices:\n",
    "        column = X[:,column_index]\n",
    "        min = np.min(column, axis=0) \n",
    "        max = np.max(column, axis=0)\n",
    "        difference = max- min\n",
    "        X[:,column_index] = (column - min) /difference\n",
    "    return X\n",
    "\n",
    "def write_to_csv_file(pred_Y, predicted_Y_file_name):\n",
    "    pred_Y = pred_Y.reshape(len(pred_Y), 1)\n",
    "    with open(predicted_Y_file_name, 'w', newline='') as csv_file:\n",
    "        wr = csv.writer(csv_file)\n",
    "        wr.writerows(pred_Y)\n",
    "        csv_file.close()\n",
    "\n",
    "\n",
    "def predict(test_X_file_path):\n",
    "    test_X, weights = import_data_and_weights(test_X_file_path, \"WEIGHTS_FILE.csv\")\n",
    "    real_X,_= import_data_and_weights(\"train_X_pr.csv\", \"WEIGHTS_FILE.csv\")\n",
    "    X = test_X\n",
    "    X = replace_null_values_with_mean(X,real_X)\n",
    "    print(X.shape)\n",
    "    X = convert_given_cols_to_one_hot(X, [0,3])\n",
    "    print(X.shape)\n",
    "    m=(X.shape)[0]\n",
    "    n=(X.shape)[1]\n",
    "    col=[]\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if X[j][i]>1 or X[j][i]<0:\n",
    "                col.append(i)\n",
    "                break\n",
    "    X = min_max_normalize(X, col)\n",
    "    #corr_mat = get_correlation_matrix(X)\n",
    "    #fea = select_features(corr_mat, 0, 0.6)\n",
    "    fea = [1, 2, 3, 4, 5, 7, 8, 9, 10]\n",
    "    print(X.shape)\n",
    "    count=0\n",
    "    for i in range(n):\n",
    "        if i not in fea:\n",
    "            X=np.delete(X,i-count,axis=1)\n",
    "            count+=1\n",
    "    test_X = X\n",
    "    pred_Y = predict_target_values(test_X, weights)\n",
    "    write_to_csv_file(pred_Y, \"predicted_test_Y_pr.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #test_X_file_path = sys.argv[1]\n",
    "    test_X_file_path=\"X_FILE.csv\"\n",
    "    predict(test_X_file_path)\n",
    "    # Uncomment to test on the training data\n",
    "    validate(test_X_file_path, actual_test_Y_file_path=\"Y_FILE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 7)\n",
      "(712, 11)\n",
      "(712, 11)\n",
      "(712, 9)\n",
      "[ 0.33212241 -0.50802577  0.37157664 -0.12121541  0.05964252 -0.10215886\n",
      "  0.01382647 -0.0057703   0.48489211  0.3408232 ]\n",
      "Weighted F1 score 0.4735388444545872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from validate import validate\n",
    "\n",
    "\"\"\"\n",
    "Predicts the target values for data in the file at 'test_X_file_path', using the weights learned during training.\n",
    "Writes the predicted values to the file named \"predicted_test_Y_pr.csv\". It should be created in the same directory where this code file is present.\n",
    "This code is provided to help you get started and is NOT a complete implementation. Modify it based on the requirements of the project.\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    return s\n",
    "\n",
    "def import_data_and_weights(test_X_file_path, weights_file_path):\n",
    "    test_X = np.genfromtxt(test_X_file_path, delimiter=',', dtype=np.float64, skip_header=1)\n",
    "    weights = np.genfromtxt(weights_file_path, delimiter=',', dtype=np.float64)\n",
    "    return test_X, weights\n",
    "\n",
    "def apply_one_hot_encoding(X):\n",
    "    unique_values = list(set(X))\n",
    "    unique_values.sort()\n",
    "    one_hot_encoding_map = {}\n",
    "    counter = 0\n",
    "    for x in unique_values:\n",
    "        one_hot_encoding_map[x] = [0 for i in range(len(unique_values))]\n",
    "        one_hot_encoding_map[x][counter] = 1\n",
    "        counter += 1\n",
    "    one_hot_encoded_X = []\n",
    "    for x in X:\n",
    "        one_hot_encoded_X.append(one_hot_encoding_map[x])\n",
    "\n",
    "    one_hot_encoded_X = np.array(one_hot_encoded_X, dtype=int)\n",
    "    return one_hot_encoded_X\n",
    "\n",
    "def convert_given_cols_to_one_hot(X, column_indices):\n",
    "    one_hot_encoded_X = np.zeros([len(X),1])\n",
    "\n",
    "    start_index = 0\n",
    "    #acts column pointer in X\n",
    "\n",
    "    for curr_index in column_indices:\n",
    "        #adding the columns present before curr_index in X (and not present in one_hot_encoded_X), to one_hot_encoded_X\n",
    "        one_hot_encoded_X=np.append(one_hot_encoded_X,X[:, start_index:curr_index], axis=1)\n",
    "        \n",
    "        #applying one hot encoding for current column\n",
    "        one_hot_encoded_column = apply_one_hot_encoding(X[:,curr_index])\n",
    "\n",
    "        #appending the obtained one hot encoded array to one_hot_encoded_X\n",
    "        one_hot_encoded_X=np.append(one_hot_encoded_X,one_hot_encoded_column, axis=1)\n",
    "\n",
    "        #moving the column pointer of X to next current_index\n",
    "        start_index = curr_index+1\n",
    "\n",
    "    #adding any remaining columns to one_hot_encoded_X    \n",
    "    one_hot_encoded_X=np.append(one_hot_encoded_X,X[:,start_index:], axis=1)\n",
    "    one_hot_encoded_X = one_hot_encoded_X[:,1:]\n",
    "    return one_hot_encoded_X\n",
    "\n",
    "def get_correlation_matrix(X):\n",
    "    num_vars = len(X[0])\n",
    "    m = len(X)\n",
    "    correlation_matix = np.zeros((num_vars,num_vars))\n",
    "    for i in range(0,num_vars):\n",
    "        for j in range(i,num_vars):\n",
    "            mean_i = np.mean(X[:,i])\n",
    "            mean_j = np.mean(X[:,j])\n",
    "            std_dev_i = np.std(X[:,i])\n",
    "            std_dev_j = np.std(X[:,j])\n",
    "            numerator = np.sum((X[:,i]-mean_i)*(X[:,j]-mean_j))\n",
    "            denominator = (m)*(std_dev_i)*(std_dev_j)\n",
    "            corr_i_j = numerator/denominator    \n",
    "            correlation_matix[i][j] = corr_i_j\n",
    "            correlation_matix[j][i] = corr_i_j\n",
    "    return correlation_matix\n",
    "\n",
    "def select_features(corr_mat, T1, T2):\n",
    "    n=len(corr_mat)\n",
    "    filtered_features = []\n",
    "    for i in range(1,n):\n",
    "        if (abs(corr_mat[i][0]) > T1):\n",
    "            filtered_features.append(i)\n",
    "    m = len(filtered_features)\n",
    "    removed_features = []\n",
    "    selected_features = list(filtered_features)\n",
    "    for i in range(0,m):\n",
    "        for j in range(i+1,m):\n",
    "            f1 = filtered_features[i]\n",
    "            f2 = filtered_features[j]\n",
    "            if (f1 not in removed_features and f2 not in removed_features): \n",
    "                if (abs(corr_mat[f1][f2]) > T2):\n",
    "                    selected_features.remove(f2)\n",
    "                    removed_features.append(f2)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "def replace_null_values_with_mean(X,real_X):\n",
    "    #Obtain mean of columns\n",
    "    col_mean = np.nanmean(real_X, axis=0)\n",
    "\n",
    "    #Find indicies that we need to replace\n",
    "    inds = np.where(np.isnan(X))\n",
    "\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    X[inds] = np.take(col_mean, inds[1])\n",
    "    return X\n",
    "\n",
    "def mean_normalize(X, column_indices):\n",
    "    X[:,column_indices]=((X[:,column_indices]-np.mean(X,axis=0)[column_indices])/((np.max(X,axis=0)[column_indices]-np.min(X,axis=0)[column_indices])))\n",
    "    return X\n",
    "\n",
    "def predict_target_values(test_X, weights):\n",
    "    k=[]\n",
    "    s=[]\n",
    "    print(weights[0][:])\n",
    "    for i in range(2):\n",
    "        b=weights[i][-1]\n",
    "        Z=np.dot(test_X,weights[i][:-1])+b\n",
    "        A=sigmoid(Z)\n",
    "        k.append(A)\n",
    "    k=np.array(k)\n",
    "    size=(k.shape)[1]\n",
    "    for i in range(size):\n",
    "        m=max(k[0][i],k[1][i])\n",
    "        if m==k[0][i]:\n",
    "            s.append([0])\n",
    "        else:\n",
    "            s.append([1])\n",
    "    s=np.array(s)\n",
    "    return s\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    The preprocessing techniques which are used on the train data, should also be applied on the test \n",
    "    1. The feature scaling technique used on the training data should be applied as it is (with same mean/standard deviation/min/max) on the test data as well.\n",
    "    2. The one-hot encoding mapping applied on the train data should also be applied on test data during prediction.\n",
    "    3. During training, you have to write any such values (mentioned in above points) to a file, so that they can be used for prediction.\n",
    "     \n",
    "    You can load the weights/parameters and the above mentioned preprocessing parameters, by reading them from a csv file which is present in the preprocessing_regularization.zip\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict Target Variables\n",
    "    \"\"\"\n",
    "    You can make use of any other helper functions which might be needed.\n",
    "    Make sure all such functions are submitted in preprocessing_regularization.zip and imported properly.\n",
    "    \"\"\"\n",
    "def min_max_normalize(X, column_indices):\n",
    "    for column_index in column_indices:\n",
    "        column = X[:,column_index]\n",
    "        min = np.min(column, axis=0) \n",
    "        max = np.max(column, axis=0)\n",
    "        difference = max- min\n",
    "        X[:,column_index] = (column - min) /difference\n",
    "    return X\n",
    "\n",
    "def write_to_csv_file(pred_Y, predicted_Y_file_name):\n",
    "    pred_Y = pred_Y.reshape(len(pred_Y), 1)\n",
    "    with open(predicted_Y_file_name, 'w', newline='') as csv_file:\n",
    "        wr = csv.writer(csv_file)\n",
    "        wr.writerows(pred_Y)\n",
    "        csv_file.close()\n",
    "\n",
    "\n",
    "def predict(test_X_file_path):\n",
    "    test_X, weights = import_data_and_weights(test_X_file_path, \"WEIGHTS_FILE.csv\")\n",
    "    real_X,_= import_data_and_weights(\"train_X_pr.csv\", \"WEIGHTS_FILE.csv\")\n",
    "    X = test_X\n",
    "    X = replace_null_values_with_mean(X,real_X)\n",
    "    print(X.shape)\n",
    "    X = convert_given_cols_to_one_hot(X, [0,3])\n",
    "    print(X.shape)\n",
    "    m=(X.shape)[0]\n",
    "    n=(X.shape)[1]\n",
    "    col=[]\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if X[j][i]>1 or X[j][i]<0:\n",
    "                col.append(i)\n",
    "                break\n",
    "    X = min_max_normalize(X, col)\n",
    "    #corr_mat = get_correlation_matrix(X)\n",
    "    fea = [1, 2, 3, 4, 5, 7, 8, 9, 10]\n",
    "    print(X.shape)\n",
    "    count=0\n",
    "    for i in range(n):\n",
    "        if i not in fea:\n",
    "            X=np.delete(X,i-count,axis=1)\n",
    "            count+=1\n",
    "    print(X.shape)\n",
    "    test_X = X\n",
    "    pred_Y = predict_target_values(test_X, weights)\n",
    "    write_to_csv_file(pred_Y, \"predicted_test_Y_pr.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #test_X_file_path = sys.argv[1]\n",
    "    test_X_file_path=\"train_X_pr.csv\"\n",
    "    predict(test_X_file_path)\n",
    "    # Uncomment to test on the training data\n",
    "    validate(test_X_file_path, actual_test_Y_file_path=\"train_Y_pr.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
